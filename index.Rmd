---
title: "Tópicos en Economía y Negocios usando R"
author:
- Carolina Gacitúa
- Emilio Guamán
- Giovanni Villa
date: "15 de noviembre de 2018"
output:
  html_document:
    df_print: paged
  html_notebook:
    number_sections: yes
subtitle: Tarea 5
always_allow_html: yes
urlcolor: blue
---

```{r, echo = FALSE, out.width='8%', fig.align='center', fig.cap="", fig.pos = 'h'}
knitr::include_graphics('C:/Users/Emilio/Documents/Clases R/Tarea5/escudo1.png')
```
```{r, results='asis', echo=FALSE}
cat("\\newpage")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(dplyr)
library(ggplot2)
library(WDI)
library(ggthemes)
library(gridExtra)
library(ggstance)
library(ggrepel)
library(rworldmap)
library(tmap)
library(tmaptools)
library(RColorBrewer)
library(rgdal)
library(pacman)
library(stargazer)
library(car) 
library(lmtest)
library(sandwich)
library(quantmod)
library(tidyquant)
```

#Pregunta 1 {-}

#Pregunta 2 {-}

##Parte (a) {-}

En esta parte, se pide que descarguemos el precio de las acciones de Microsofot ("MSFT") y Apple ("AAPL") desde Enero del 2000 hasta Agosto del 2018 con periodicidad mensual.

Utilizando la librería *tidyquant*, descaragamos las bases pedidas. Los códigos utilizados son los siguientes:

```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
microsoft<-tq_get("MSFT", get = "stock.prices",from = "2000-01-01",
                  to = "2018-08-01", periodicity = "monthly")
apple<-tq_get("AAPL", get = "stock.prices",from = "2000-01-01", 
              to = "2018-08-01", periodicity = "monthly")
```

Con la función *tq_get* especificamos la base pedida. Luego, con *get* pedimos dentro de las bases especificadas los precios de las acciones de la compañía respectiva. Con *from* y *to* especificamos que se nos entreguen los datos desde el 01 de enero del 2000 hasta el 01 de agosto del 2018. Finalmente, con *periodicity* pedimos que se nos entreguen datos mensuales. El *data frame* donde guardamos los datos de acciones de microsoft es *microsoft* y el otro lo llamamos *apple*. 
Luego, procedemos a generar un *data frame* llamado *df1*, que contendrá simplemente una fecha y los retornos mensuales tanto de *microsoft* como de *apple* (columna *adjusted* en ambos casos). El código utilizado es:

```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
df1<-data.frame(apple$date, microsoft$adjusted, apple$adjusted)
```

## Parte (b) {-}

A continuación, procedemos a programar una función que nos permita: (i) calcular los retornos, (ii) graficar los retornos y retornos acumulados y (iii) testear normalidad utilizando el test de Jarque-Bera (JB). El estadístico de este test está dado por las siguientes expresiones:

$$ JB = n \left( \frac{skewness^2}{6} + \frac{(kurtosis - 3)^2}{24}\right)  $$
$$ skewness = \frac{ \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^{3}}{(\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^{2})^{3/2}} $$
$$ kurtosis = \frac{ \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^{4}}{(\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^{2})^{2}} $$

Donde $JB$ se distribuye bajo una distribución chi-cuadrado con 2 grados de libertad ($JB \sim \chi^{2}_{(\alpha,2)}$). La función que haremos tendrá capacidad de aceptar $n$ precios de acciones.

La función que programaremos para hacer todo lo pedido se llama *funcion1* y tiene la siguiente sintaxis: 
$$ funcion1(x, nacc=1, retlog=TRUE, grafret=FALSE, showtest=FALSE)
 $$
El usuario puede interactuar alterando cada uno de los componentes mostrados antes. El primer ítem de la sintaxis, el *x* corresponde al *dataframe* que se va a utilizar para el cálculo de los retornos (mensuales o acumulados), los gráficos y el test de normalidad. La única restricción es que este el *dataframe* que se ponga dentro de la *funcion1* debe tener las fechas en la primera columna y los precios de las acciones en las siguientes columnas (puede tener precios de distintas acciones). El parámetro *nacc=* sirve para que el usuario fije la columna con las acciones que desea que estén sujetas a *funcion1*. El valor por defecto se ha fijado que sea la primera columna con precios de acciones del *data frame* respectivo, pero esto se puede variar cambiando el número. 

Luego, en *retlog* el usuario puede especificar que se calculen los retornos acumulados de las acciones de forma logarítmica usando *retlog=TRUE*. Esto es, que se calculen usando $ln(\frac{p_t}{p_{t-1}})$. Alternativamente, el usuario puede especificar que se calculen los retornos de las acciones de la forma normal: $\frac{p_t-p_{t-1}}{p_{t-1}}$, usando *retlog=FALSE*. El siguiente componente de la sintaxis es *grafret*, y sirve para que el usuario especifique si quiere que se grafiquen los retornos acumulados (*grafret=TRUE*) o los retornos mensuales (*grafret=TRUE*). 

Finalmente, *showtest* permite al usuario pedir que se le muestre el resultado de un test de Jarque-Bera para determinar si los datos de los precios de acciones siguen una distribución normal o no. Si el usuario quiere que se le muestre, tanto el estadístico del test como el p-value, fija la opción *showtest=TRUE*. Si no quiere que se muestre, fija *showtest=FALSE*. Recordemos que un p-value menor a 0.05 dicta que se rechaza la nula de normalidad de los datos al 5% de significancia (o si el valor del estadístico es mayor al valor crítico de la chi-cuadrado). Si el usuario solo especifica el *dataframe*, es decir, si ingresa algo así como *funcion1(df1)*, la función que programamos asume, por defecto, los valores que se muestran en la expresión (toma las acciones de la primera columna que cuente con datos de precios, calcula los retornos usando forma logarítmica, grafica retornos mensuales, etc).

Los códigos utilizados en la programación de *funcion1* se muestran a continuación.

```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
funcion1<-function(x, nacc=1, retlog=TRUE, grafret=FALSE, showtest=FALSE){
  if(retlog==TRUE){
    base<-x[,2:ncol(x)]
    n<-nrow(base)
    fecha<-x[,1]
    fecha1<-fecha[2:n]
    Z<-sapply(base, function(i){log(i[2:n]/i[1:(n-1)])})
    work<-data.frame(fecha1,Z)
    nacc1<-1+nacc
    a<-ggplot(work, aes(x=fecha1,y=cumprod(1+work[,nacc1])-1)) + geom_line() + geom_point(col="Red")+ggtitle("Retornos Mensuales Acumulados")
    b<-ggplot(work, aes(x=fecha1,y=work[,nacc1])) + geom_line() + geom_point(col="Red")+ggtitle("Retornos Mensuales")
    desvia=(work[,nacc1]-mean(work[,nacc1]))
    sumaske3=sum(desvia^3)/n
    sumaske2=(sum(desvia^2)/n)^(1.5)
    sumakur4=sum(desvia^4)/n
    sumakur2=(sum(desvia^2)/n)^2
    skew=sumaske3/sumaske2
    kurt=sumakur4/sumakur2
    jb=n*((skew^2/6)+((kurt-3)^2/24))
    pval<-1-pchisq(jb,df=2)
  }
  else{
    base<-x[,2:ncol(x)]
    n<-nrow(base)
    fecha<-x[,1]
    fecha1<-fecha[2:n]
    nacc1<-1+nacc
    Z<-sapply(base, function(i){(i[2:n]-i[1:(n-1)])/i[1:(n-1)]})
    work<-data.frame(fecha1,Z)
    a<-ggplot(work, aes(x=fecha1,y=cumprod(1+work[,nacc1])-1)) + geom_line() + geom_point(col="Red")+ggtitle("Retornos mensuales Acumulados")
    b<-ggplot(work, aes(x=fecha1,y=work[,nacc1])) + geom_line() + geom_point(col="Red")+ggtitle("Retornos mensuales")
    desvia=(work[,nacc1]-mean(work[,nacc1]))
    sumaske3=sum(desvia^3)/n
    sumaske2=(sum(desvia^2)/n)^(1.5)
    sumakur4=sum(desvia^4)/n
    sumakur2=(sum(desvia^2)/n)^2
    skew=sumaske3/sumaske2
    kurt=sumakur4/sumakur2
    jb=n*((skew^2/6)+((kurt-3)^2/24))
    pval<-1-pchisq(jb,df=2)
  }
  if(grafret==FALSE & showtest==FALSE){
    return(list(work,b))
  }
  else if(grafret==FALSE & showtest==TRUE){
    return(list(work,b, paste0("El estadístico es ",jb," y su valor p es: ", pval)))
  }
  else if(grafret==TRUE & showtest==FALSE){
    return(list(work,a))
  }
  else{
    return(list(work,a, paste0("El estadístico es ",jb," y su valor p es: ", pval))) 
  }
}
```

La programación del código para especificando con un *if* que si se fija *retlog=TRUE* (es decir, si se quiere que los retornos se calculen de forma logarítmica), se aplique con *sapply* el cálculo de los retornos a la base *x* con la fórmula de logretornos. Luego se crea el *dataframe* auxiliar *work*, donde se guardan los cálculos de los log retornos y la columna auxiliar  *nacc1*. A continuación, se define el gráfico *a*, realizado con ggplot, que grafica la *fecha* en el eje *x* y los retornos acumulados guardados en *nacc1* de *work* (con *cumprod*), usando un gráfico de líneas (*geom_line*()). Además, se define el gráfico *b* que tiene una construcción similar a *a*, salvo por que simplemente se piden los retornos guardados en *nacc1* sin el *cumprod* (retornos mensuales). 

Luego, se procede a construir el estadístico $JB$. Para ello, se generan las sumas, elevadas a distintos exponentes, de las desviaciones de la media. Luego se computa la *skewness* y la *kurtosis* respectivamente, para luego generar *jb*. Además, se construye el objeto *pval*, que entrega el pvalue de la distribución chicuadrado con 2 grados de libertad (1 menos la prob. acumulada, pues es un test de una sola cola). Acá es donde se cierra el primer paréntesis, es decir, si el usuario fija *TRUE* en *retlog*, nuestra función ejecuta todo lo descrito hasta ahora.

Para programar el caso en que el usuario fija *FALSE* en *retlog*, procedemos de una forma muy similar a lo anterior. Partimos poniendo la condición *else*, y dentro de las llaves , copiamos el mismo código que estaba en la parte anterior, con la salvedad de que varía la forma de calcular los retornos (línea en que sale *sapply*). 

Teniendo ya los gráficos generados y el test de $JB$ calculado, las siguiente líneas contienen *if* que van condicionando qué output mostrar según lo pedido por el usuario. Así, si se fija *FALSE* tanto en *grafret* como en *showtest*, se pide que con *return* se muestren los cálculos de los retornos y se muestre el gráfico *b* (retornos mensuales). Luego, se ponde un *else if*, fijando que si el usuario pone *FALSE* en *grafret* pero *TRUE* en *showtest*, se muestran los cálculos, el gráfico *b* y la siguiente línea (fijada con paste): 
*El estadístico es "jb" y su valor p es: ", pval*. Las siguientes líneas siguen examinando las variantes restantes, y van condicionando el output mostrado según eso.

Para finalizar, a continuación mostramos una aplicación de nuestra función. Supondremos que se quiere calcular los retornos en logaritmo de la data acciones de *Apple* contenidas en *df1*, y que se quieren ver los retornos acumulados y el test de Jarque Bera. El código que debiese utilizar un usuario interesado en esto es el siguiente:

```{r echo=TRUE, message=FALSE, warning=FALSE, results = "asis"}
funcion1(df1, nacc=2, retlog=TRUE, grafret=TRUE, showtest=TRUE)
```

#Pregunta 3 {-}

En esta pregunta se nos pide suponer el siguiente modelo poblacional:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u \quad (1) $$
No obstante, se señala que por ignorancia o falta de datos, se termina estimando el siguiente modelo:
 $$y = \beta_0 + \beta_1 x_1 + v \quad (2) $$
Con $v = \beta_2 x_2 + u$. Luego se nos proporcionan instrucciones para generar los distintos parámetros y variables de los modelos descritos. En particular, el número de repeticiones es $R=10000$, los tamaños muestrales a trabajar son $n=50,100,500,1000$. Los valores "verdaderos" de los parámetros son: $\beta_0 = 2, \beta_1 = 2,5, \beta_2 = 1$. El término de error $u$ tiene una distribución $u\sim N[0,1]$ (normal estándar). 

## Parte (a) {-}

En esta parte y en la parte b,, asumimos $x_2 = 0,8x_1 + e$, donde $x_1 \sim N(20,1)$ y donde $e$ es una perturbación que se distribuye $e \sim N(0,1)$. Con esto en mente, se nos pide calcular las esperanzas de los coeficientes $\beta_0$ y $\beta_0$ y sus varianzas para los cuatro tamaños muestrales mencionados antes. Además, se nos pide decir si existe o no sesgo y cual es la trayectoria de este a medida que la muestra aumenta (si es que disminuye o no).

Para responder la pregunta, partimos generando las variables de acuerdo a lo especificado en el enunciado. Los códigos utilizados son:

```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed(1234567)
R=10000
n=c(50,100,500,1000) 
betas_0=matrix(NA,nrow = R, ncol=4) 
betas_1=matrix(NA,nrow = R, ncol=4) 
beta0=2
beta1=2.5
beta2=1.0
beta3=0.8
```
El comando *set.seed* nos permite garantizar la replicabilidad de nuestros resultados al elegir la semilla a partir de la cual se generan los números pseudo-aleatorios pedidos. Guardamos en *n* los tamaños muestrales pedidos. Generamos las matrices *betas_0* y *betas_1*, de 10000 filas y 4 columnas, para ir guardando los coeficientes estimados de la regresión lineal indicada en la ecuación (2). Luego, fijamos los valores verdaderos de los parámetros. *beta3* no es un parámetro que salga en los modelos, pero es un parámetro adicional que generamos solamente para poder generar $x_2$ según lo pedido en esta parte. A continuación, programamos un for para que estime 10000 veces la ecuación (2) y guarde los coeficientes de los betas en las matrices ya descritas. Los códigos son:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
for (j in 1:length(n)) {
  x_1=rnorm(n[j],20,1) 
  for (i in 1:R) {
    e=rnorm(n[j],0,1)
    x_2=beta3*x_1+e
    u=rnorm(n[j],0,1)
    v=beta2*x_2+u
    y=beta0+beta1*x_1+v
    modelo=lm(y~x_1) 
    betas_0[i,j]=modelo$coef[1]
    betas_1[i,j]=modelo$coef[2]
  }
}
```
El for de la primera fila, pide que se generen, para los cuatro tamaño de muestras pedidos, los valores de la variable $x_1$, que sigue una distribución normal con media 20 y desviación estándar 1. El segundo for, programa 10000 repeticiones (por eso sale *in 1:R*), en las que se generan $e$, $x_2$ $u$ $v$ e $y$. Además, en ese mismo código se guardan las estimaciones por mínimos cuadrados ordinarios (MCO) en *modelo*. La regresión de $y$ contra $x_1$ se realiza con el comando *lm*. Finalmente, las dos última líneas guardan los coeficientes de la regresión en las matrices antes definidas. A continuación, programamos el cálculo de la esperanza, la varianza y el sesgo (definido como la diferencia en valor absoluto entre la esperanza del coeficiente y su valor problacional), para $\hat{\beta}_0$ en los cuatro tamaños muestrales. Los códigos son:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
#n=50
E1_beta0 = mean(betas_0[,1])
V1_beta0 = var(betas_0[,1])
sesgo_01=abs(mean(betas_0[,1])-beta0)
sesgo_01

#n=100
E2_beta0 = mean(betas_0[,2])
V2_beta0 = var(betas_0[,2])
sesgo_02=abs(mean(betas_0[,2])-beta0)
sesgo_02

#n=500
E3_beta0 = mean(betas_0[,3])
V3_beta0 = var(betas_0[,3])
sesgo_03=abs(mean(betas_0[,3])-beta0)
sesgo_03

#n=1000
E4_beta0 = mean(betas_0[,4])
V4_beta0 = var(betas_0[,4])
sesgo_04=abs(mean(betas_0[,4])-beta0)
sesgo_04
```
No se muestran acá los resultados porque se presentará una tabla al final de esta parte. La primera línea de cada grupo (que sale bajo cada tamaño muestral *#n*) calcula la esperanza de $\hat{\beta}_0$ con el comando *mean*. Se pide columna de *betas_0* donde se encuentran guardadas las estimaciones de $\hat{\beta}_0$ para el tamaño de muestra respectivo. La segunda línea calcula la varianza del coeficiente con *var*. Finalmente, en la tercera línea generamos el sesgo del estimador al aplicar el valor absoluto (función *abs*) a la resta entre la esperanza de las estimaciones y el valor verdadero del parámetro (en este caso, $\beta_0 = 2$). Repetimos el mismo ejercicio, pero para recurperar las estimaciones de $\hat{\beta}_1$, con los siguiente códigos:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
#n=50
E1_beta1 = mean(betas_1[,1])
V1_beta1 = var(betas_1[,1])
sesgo_11=abs(mean(betas_1[,1])-beta1)
sesgo_11


#n=100
E2_beta1 = mean(betas_1[,2])
V2_beta1 = var(betas_1[,2])
sesgo_12=abs(mean(betas_1[,2])-beta1)
sesgo_12

#n=500
E3_beta1 = mean(betas_1[,3])
V3_beta1 = var(betas_1[,3])
sesgo_13=abs(mean(betas_1[,3])-beta1)
sesgo_13

#n=1000
E4_beta1 = mean(betas_1[,4])
V4_beta1 = var(betas_1[,4])
sesgo_14=abs(beta1-mean(betas_1[,4]))
sesgo_14
```
Para finalizar, compilaremos todos los resultados en una solo *data frame* llamado *a*. Este *dataframe*, de 8 filas y 6 columnas, muestra el parámetro considerado, el tamaño de muestra de la estimación, el valor poblacional del parámetro, la esperanza del coeficiente en las 10000 repeticiones, la varianza y el sesgo. Los códigos y el *data frame* son:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "asis"}
a <- data.frame("Parámetro"= c("Beta 0", "Beta 0", "Beta 0", "Beta 0",
                               "Beta 1", "Beta 1", "Beta 1", "Beta 1"),
                "n" = c(n, n) , 
                "Valor pob. beta" = c(beta0, beta0, beta0, beta0,
                                      beta1, beta1, beta1, beta1),
                "Esperanza" = c(E1_beta0, E2_beta0, E3_beta0,
                                E4_beta0,
                          E1_beta1, E2_beta1, E3_beta1, E4_beta1),
                "Varianza" = c(V1_beta0, V2_beta0, V3_beta0,
                               V4_beta0,
                            V1_beta1, V2_beta1, V3_beta1, V4_beta1),
                "Sesgo" = c(sesgo_01, sesgo_02, sesgo_03, sesgo_04,
                            sesgo_11, sesgo_12, sesgo_13, sesgo_14)
                )
print(a)
```

La generación de *a* viene simplemente de crear con *data.frame* una tabla que vaya agregando columnas y pegando los valores ya creados antes. Como se puede ver, para ningún tamaño muestral se da que $\hat{beta}_0$ o $\hat{\beta}_1$ coinciden exactamente con su valor poblacional. Esto tiene sentido, pues sabemos que hay sesgo por omisión de variable relevante ($x_2$ está excluida de la regresión).

El sesgo es menor 1 en todos los casos. Con respecto a la trayectoria del sesgo al aumentar el tamaño muestral, se observa como al pasar de *n=50* a *n=100*, el sesgo de $\beta_0$ y de $\beta_1$ aumenta levemente. Para el caso de $\hat{\beta}_0$, el sesgo alcanza su mínimo con *n=500*. Para $\hat{\beta}_1$, el sesgo disminuye levemente al pasar a tamaños muestrales más grandes. No se observa entonces un patrón claro entre tamaño muestral y sesgo. Esto tiene sentido si pensamos que aumentar el tamaño de la muestra no solucionará el problema que tenemos de variable relevante omitida. Por ende, el sesgo permanecerá mientras no se corrija esto añadiendo alguna proxy de $x_2$ al modelo. Aún así, sí observamos un menor sesgo en tamaños muestrales grandes para ambos coeficientes. Llama la atención la alta varianza que presenta $\hat{\beta}_0$ con *n* bajos, algo que creemos que tiene que ver simplemente con el carácter aleatorio de los datos.


##Parte (b) {-}

En esta parte, se nos pide graficar la distribución de los $\hat{\beta}_1$ calculados en la parte anterior. Como vamos a generar un gráfico de densidad, en primer lugar crearemos *dataframes* auxiliares para poder obtener la curva de la normal que pegaremos en el gráfico de cada caso. Utilizamos los siguientes códigos:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
c1 <- seq(min(betas_1[,1]), max(betas_1[,1]), length=R)
densidad_normal1 <- data.frame(c1=c1, f1=dnorm(c1, mean(betas_1[,1]), sd(betas_1[,1])))

c2 <- seq(min(betas_1[,2]), max(betas_1[,2]), length=R)
densidad_normal2 <- data.frame(c2=c2, f2=dnorm(c2, mean(betas_1[,2]), sd(betas_1[,2])))

c3 <- seq(min(betas_1[,3]), max(betas_1[,3]), length=R)
densidad_normal3 <- data.frame(c3=c3, f3=dnorm(c3, mean(betas_1[,3]), sd(betas_1[,3])))

c4 <- seq(min(betas_1[,4]), max(betas_1[,4]), length=R)
densidad_normal4 <- data.frame(c4=c4, f4=dnorm(c4, mean(betas_1[,4]), sd(betas_1[,4])))
```
Primero vamos guardando en las secuencias llamadas $g_i, \quad i=1,2,3,4$ los coeficientes $\hat{\beta}_1$ para cada tamaño muestral. A continuación, en los *data frames* $\text{densidad_normal}_i, \quad i=1,2,3,4$ vamos generando la curva normal respectiva a partir de la función *dnorm*, donde fijamos la esperanza de los betas y su desviación estándar. Una vez realizado esto, procedemos a graficar las distribuciones para los distintos tamaños muestrales con los siguientes códigos:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'asis', fig.align='center', fig.width=8, fig.height=6}
#n=50
graph_beta11 <- data.frame(B11=betas_1[,1]) %>% ggplot(aes(betas_1[,1], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normal1, mapping = aes(c1,f1), color = "red")+
  ggtitle("n=50")+xlab(expression(hat(beta)[1]))+
  theme_economist()

#n=100
graph_beta12 <- data.frame(B12=betas_1[,2]) %>% ggplot(aes(betas_1[,2], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normal2, mapping = aes(c2,f2), color = "red")+
  ggtitle("n=100")+xlab(expression(hat(beta)[1]))+
  theme_economist()

#n=500
graph_beta13 <- data.frame(B11=betas_1[,3]) %>% ggplot(aes(betas_1[,3], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normal3, mapping = aes(c3,f3), color = "red")+
  ggtitle("n=500")+xlab(expression(hat(beta)[1]))+
  theme_economist()

#n=1000
graph_beta14 <- data.frame(B14=betas_1[,4]) %>% ggplot(aes(betas_1[,4], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normal4, mapping = aes(c4,f4), color = "red")+
  ggtitle("n=1000")+xlab(expression(hat(beta)[1]))+
  theme_economist()

grid.arrange(graph_beta11,graph_beta12,graph_beta13,graph_beta14)
```
En el chunk anterior, vamos generando cada gráfico a partir de la columna de *betas_1* respectiva. Con *ggplot* creamos el histograma y pedimos que se nos muestre la densidad. Luego le agregamos la curva normal a partir de *geom_line* y especificando $\text{densidad_normal}_{i}$ según corresponda. El resto de líneas sirven para agregar títulos, etiquetas a los ejes y fijar el *theme* del gráfico. Una vez generados los cuatro gráficos, los juntamos en una sola imagen con *grid.arrange*.

De la imagen anterior puede verse que la distribución de los $\hat{\beta}_1$ se asemeja bastante a una distribución normal en los cuatro casos. Ahora, como es de esperar, a medida que aumentamos el tamaño muestral, cada vez más coeficientes de los 10000 guardados se van concentrando en torno a un solo valor (3.3) y las colas de la distribución se van haciendo más cortas. Nótese que 3.3 no es el valor verdadero del parámetro poblacional $\beta_1$ (es 3.5), lo que da cuenta de la persistencia del sesgo por variable omitida relevante (al excluir $x_2$ de la regresión).

##Parte (c) {-}

Ahora se nos pide que veamos como cambian los resultados obtenidos en las partes (a) y (b) bajo el supuesto de que $\x_2 \sim U[0,1]$. Para ello, vamos a volver a programar el *for* de la parte (a) bajo este nuevo supuesto. Los códigos utilizados son los siguientes:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed(1234567)
R=10000
n=c(50,100,500,1000) 
betas_u0=matrix(NA,nrow = R, ncol=4)
betas_u1=matrix(NA,nrow = R, ncol=4) 
beta0=2
beta1=2.5
beta2=1.0

for (j in 1:length(n)) {
  
  x_1=rnorm(n[j],20,1) 
  for (i in 1:R) {
    x_2=runif(n[j],0,1)
    u=rnorm(n[j],0,1)
    v=beta2*x_2+u
    y=beta0+beta1*x_1+v
    modelo=lm(y~x_1) 
    betas_u0[i,j]=modelo$coef[1]
    betas_u1[i,j]=modelo$coef[2]
  }
}
```
La parte anterior al *for* no cambia, salvo por la creación de los nuevos *dataframes* *betas_u0* y *betas_u1* para guardar los coeficientes de las nuevas estimaciones. De ahí hacia abajo, lo único que cambia respecto a la parte (a) es la generación de $x_2$. A continuación, procedemos a calcular la esperanza, la varianza y el sesgo (definido como la diferencia en valor absoluto de la esperanza del coeficiente y su valor poblacional). Los códigos son los mismos que en la parte (a), salvo porque cambian los nombres con los que vamos guardando los cálculos:

```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
#Para el caso de B0

#n=50
E1u_beta0 = mean(betas_u0[,1])
V1u_beta0 = var(betas_u0[,1])
sesgo_u01=abs(mean(betas_u0[,1])-beta0)
sesgo_u01

#n=100
E2u_beta0 = mean(betas_u0[,2])
V2u_beta0 = var(betas_u0[,2])
sesgo_u02=abs(mean(betas_u0[,2])-beta0)
sesgo_u02

#n=500
E3u_beta0 = mean(betas_u0[,3])
V3u_beta0 = var(betas_u0[,3])
sesgo_u03=abs(mean(betas_u0[,3])-beta0)
sesgo_u03

#n=1000
E4u_beta0 = mean(betas_u0[,4])
V4u_beta0 = var(betas_u0[,4])
sesgo_u04=abs(mean(betas_u0[,4])-beta0)
sesgo_u04

#Para el caso de b1

#n=50
E1u_beta1= mean(betas_u1[,1])
V1u_beta1= var(betas_u1[,1])
sesgo_u11=abs(mean(betas_u1[,1])-beta1)
#definimos el sesgo como la diferencia en valor absoluto del valor b1 y la media del experimento
sesgo_u11

#n=100
E2u_beta1 = mean(betas_u1[,2])
V2u_beta1 = var(betas_u1[,2])
sesgo_u12=abs(mean(betas_u1[,2])-beta1)
sesgo_u12

#n=500
E3u_beta1 = mean(betas_u1[,3])
V3u_beta1 = var(betas_u1[,3])
sesgo_u13=abs(mean(betas_u1[,3])-beta1)
sesgo_u13

#n=1000
E4u_beta1 = mean(betas_u1[,4])
V4u_beta1 = var(betas_u1[,4])
sesgo_u14=abs(mean(betas_u1[,4])-beta1)
sesgo_u14
```
Ahora procedemos a crear la tabla resumen que presentamos en la parte (a) bajo el nuevo supuesto. Esta se llama *tab2* y se muestra a continuación en conjunto con los códigos:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "asis"}
tab2 <- data.frame("Parámetro"= c("Beta 0", "Beta 0", "Beta 0", "Beta 0",
                               "Beta 1", "Beta 1", "Beta 1", "Beta 1"),
                "n" = c(n, n) , 
                "Valor pob. beta" = c(beta0, beta0, beta0, beta0,
                                      beta1, beta1, beta1, beta1),
                "E()" = c(E1u_beta0, E2u_beta0, E3u_beta0, E4u_beta0,
                          E1u_beta1, E2u_beta1, E3u_beta1, E4u_beta1),
                "Var()" = c(V1u_beta0, V2u_beta0, V3u_beta0, V4u_beta0,
                            V1u_beta1, V2u_beta1, V3u_beta1, V4u_beta1),
                "Sesgo" = c(sesgo_u01, sesgo_u02, sesgo_u03, sesgo_u04,
                            sesgo_u11, sesgo_u12, sesgo_u13, sesgo_u14)
                )
print(tab2)
```
En general, podemos observar que, aunque obviamente persiste el sesgo en todos los casos, el sesgo bajo el nuevo supuesto aparenta ser bastante menor (tanto para $\hat{\beta}_0$ como para $\hat{\beta}_1$). Nuevamente, el sesgo no muestra una trayectoria clara al aumenta el tamaño muestral (por la persistencia en todos los casos del problema de variable omitida relevante). La razón del menor sesgo bajo el nuevo supuesto no es clara, pero quizá una intuición tiene que ver con que ahora $x_{2}$ ya no se relaciona directamente con $x_{1}$, por lo que en la construcción de los valores verdaderos de $y$ se alejaría menos de los predichos por la regresión al estar $x_{2}$ compuesta por menos "distorsiones" (no hay término de error tampoco). 

A continuación, procedemos a generar los gráficos de la distribución de $\hat{\beta}_{1}$ bajo este nuevo supuesto. Replicaremos exactamente el mismo procedimiento seguido en la parte (b). Esto es, primero generamos *dataframes* auxiliares para crear las curvas normales que luego pegaremos en los gráficos de cada caso. Los códigos para hacerlo se muestran abajo:
```{r echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
cu1 <- seq(min(betas_u1[,1]), max(betas_u1[,1]), length=R)
densidad_normalu1 <- data.frame(cu1=cu1, fu1=dnorm(cu1, mean(betas_u1[,1]), sd(betas_u1[,1])))

cu2 <- seq(min(betas_u1[,2]), max(betas_u1[,2]), length=R)
densidad_normalu2 <- data.frame(cu2=cu2, fu2=dnorm(cu2, mean(betas_u1[,2]), sd(betas_u1[,2])))

cu3 <- seq(min(betas_u1[,3]), max(betas_u1[,3]), length=R)
densidad_normalu3 <- data.frame(cu3=cu3, fu3=dnorm(cu3, mean(betas_u1[,3]), sd(betas_u1[,3])))

cu4 <- seq(min(betas_u1[,4]), max(betas_u1[,4]), length=R)
densidad_normalu4 <- data.frame(cu4=cu4, fu4=dnorm(cu4, mean(betas_u1[,4]), sd(betas_u1[,4])))
```
Luego, procedemos a crear los gráficos y a juntarlos todos en una imagen con *grid.arrange*. Los códigos son equivalente a la parte (b) y se muestran a continuación, en conjunto con los gráficos.
```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'asis', fig.align='center', fig.width=8, fig.height=6}
#n=50
graph_betau1 <- data.frame(Bu1=betas_u1[,1]) %>% ggplot(aes(betas_u1[,1], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normalu1, mapping = aes(cu1,fu1), color = "red")+
  ggtitle("n=50")+xlab(expression(hat(beta)[1]))+
  theme_economist()


#n=100
graph_betau2 <- data.frame(Bu2=betas_u1[,2]) %>% ggplot(aes(betas_u1[,2], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normalu2, mapping = aes(cu2,fu2), color = "red")+
  ggtitle("n=100")+xlab(expression(hat(beta)[1]))+
  theme_economist()


#n=500
graph_betau3 <- data.frame(Bu3=betas_u1[,3]) %>% ggplot(aes(betas_u1[,3], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normalu3, mapping = aes(cu3,fu3), color = "red")+
  ggtitle("n=500")+xlab(expression(hat(beta)[1]))+
  theme_economist()


#n=1000
graph_betau4 <- data.frame(Bu4=betas_u1[,4]) %>% ggplot(aes(betas_u1[,4], ..density..))+
  geom_histogram(color = "black", bins = 30)+
  geom_line(data = densidad_normalu4, mapping = aes(cu4,fu4), color = "red")+
  ggtitle("n=1000")+xlab(expression(hat(beta)[1]))+
  theme_economist()

grid.arrange(graph_betau1,graph_betau2,graph_betau3,graph_betau4)
```
Se observa nuevamente que a medida que aumenta el tamaño muestral, las colas de la distribución del coeficiente van disminuyendo. En este caso, al aumentar el tamaño muestral, cada vez más betas estimados se agrupan en torno a valores cercanos a 2.5 (si bien no de forma exacta, pues hay sesgo por variable omitida relevante). Vemos que para *n=1000*, la mayor densidad la contiene el valor 2.49, no el 2.5. Los gráficos acá tienen una forma bastante parecida a la distribución normal.